
# Local Docker + Ceph + Flocker Cluster with Vagrant

## STILL EXPERIMENTAL!

### What you will need

- Vagrant
- Virtialbox

## How to use this repository

Getting started
```
vagrant plugin install vai
git clone [this repo]
cd [this repo]
brew install ansible
vagrant plugin install vai
```

Next
```
git clone https://github.com/ceph/ceph-ansible.git
cd  ceph-ansible 
ansible-galaxy install ClusterHQ.flocker -p ../roles
ansible-galaxy install marvinpinto.docker -p ./roles
cp  group_vars/mons.sample  group_vars/mons
cp  ../osds.group_vars  group_vars/osds
cp  ../all.group_vars  group_vars/all
cp  ../ansible.cfg  .
cp  ../site.yml .
mv Vagrantfile Vagrantfile.original
cp ../Vagrantfile Vagrantfile
```

Create and Provision everything
```
vagrant up --provider=virtualbox
```

**or**

Create but don't provision, provision later.
```
vagrant up --no-provision --provider=virtualbox
vagrant provision
```

What is running behind the scenes is this command, you can re run it after `vagrant up` for re-runs.
```
ansible-playbook -i ansible/inventory/vagrant_ansible_inventory site.yml \
   --extra-vars "fsid=4a158d27-f750-41d5-9e7f-26ce4c9d2d45 \
   monitor_secret=AQAWqilTCDh7CBAAawXt6kyTgLFCxSvJhTEmuw== \
   flocker_agent_yml_path=${PWD}/../agent.yml"
```

In either case, an inventory is used, and should be written to `ansible/inventory`
```
$ cat ansible/inventory/vagrant_ansible_inventory
# Generated by Vagrant

ceph1 ansible_ssh_host=127.0.0.1 ansible_ssh_port=2222 ansible_ssh_private_key_file=/Users/<USER>/Desktop/path-to/flocker-ceph-vagrant/ceph-ansible/.vagrant/machines/ceph1/virtualbox/private_key ansible_ssh_user=vagrant
ceph2 ansible_ssh_host=127.0.0.1 ansible_ssh_port=2200 ansible_ssh_private_key_file=/Users/<USER>/Desktop/path-to/flocker-ceph-vagrant/ceph-ansible/.vagrant/machines/ceph2/virtualbox/private_key ansible_ssh_user=vagrant
ceph3 ansible_ssh_host=127.0.0.1 ansible_ssh_port=2201 ansible_ssh_private_key_file=/Users/<USER>/Desktop/path-to/flocker-ceph-vagrant/ceph-ansible/.vagrant/machines/ceph3/virtualbox/private_key ansible_ssh_user=vagrant
ceph4 ansible_ssh_host=127.0.0.1 ansible_ssh_port=2202 ansible_ssh_private_key_file=/Users/<USER>/Desktop/path-to/flocker-ceph-vagrant/ceph-ansible/.vagrant/machines/ceph4/virtualbox/private_key ansible_ssh_user=vagrant

[osds]
ceph2
ceph3
ceph4

[mons]
ceph1
ceph2
ceph3

[mdss]
ceph1

[rdgws]
ceph1
```

Also in either case, once complete, a healthy ceph cluster should exist
```
vagrant ssh ceph1 -c "sudo ceph -s"
    cluster 4a158d27-f750-41d5-9e7f-26ce4c9d2d45
     health HEALTH_OK
     monmap e1: 3 mons at {ceph1=192.168.5.2:6789/0,ceph2=192.168.5.3:6789/0,ceph3=192.168.5.4:6789/0}
            election epoch 4, quorum 0,1,2 ceph1,ceph2,ceph3
     mdsmap e6: 1/1/1 up {0=ceph1=up:active}
     osdmap e20: 6 osds: 6 up, 6 in
            flags sortbitwise
      pgmap v30: 320 pgs, 3 pools, 1960 bytes data, 20 objects
            221 MB used, 5887 MB / 6109 MB avail
                 320 active+clean
  client io 2030 B/s wr, 17 op/s
```

Check your Flocker Cluster
```
vagrant ssh ceph1
sudo su
curl --cacert /etc/flocker/cluster.crt  --cert /etc/flocker/plugin.crt --key /etc/flocker/plugin.key --header "Content-type: application/json" https://ceph1:4523/v1/state/nodes | python -m json.tool
[
    {
        "host": "192.168.5.5",
        "uuid": "3aab9ca0-a48e-4beb-8f8c-4e814f8cecf8"
    },
    {
        "host": "192.168.5.3",
        "uuid": "2a00f5c9-b843-41c7-adf4-99013d09a594"
    },
    {
        "host": "192.168.5.4",
        "uuid": "ac3cc217-7706-447b-b1ff-a0c774d95c6b"
    }
]
```

## License 

MIT / BSD
